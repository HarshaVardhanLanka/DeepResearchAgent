ğŸ¤– Open Deep Research Agent
An Autonomous AI Researcher powered by LangGraph, Tavily, and OpenRouter.
Streamlit App

ğŸ“– Overview
The Open Deep Research Agent is not just a chatbotâ€”it is an autonomous research assistant designed to perform in-depth web analysis and document synthesis. Unlike standard LLMs that rely solely on training data, this agent actively browses the web, verifies facts, and compiles structured reports with citations.

It features a Memory System backed by MongoDB to store your research history and supports PDF Analysis to combine internal documents with external web knowledge.

ğŸ—ï¸ Architecture & Workflow
The system is built on a State Graph architecture using LangGraph. It treats the research process as a pipeline of specialized agents passing a "State" object between them.

Architecture Diagram
(Replace the link above with your actual diagram image path)

ğŸ”„ The Workflow
The agent follows a cyclical workflow to ensure high-quality output:

mermaid

graph LR
    User(User Input) --> Planner
    Planner(ğŸ§  Planner Agent) -->|Generates Queries| Searcher
    Searcher(ğŸ” Searcher Agent) -->|Web Results| Writer
    Writer(âœï¸ Writer Agent) -->|Final Report| Output
    Output -->|Save| MongoDB
User Input: The user provides a topic or uploads a PDF.
Planner: Analyzes the request to determine if it's a new topic or a follow-up. Generates targeted search queries.
Searcher: Executes the search queries using the Tavily API to gather real-time, verified sources.
Writer: Synthesizes the search results and PDF context into a coherent, cited report.
Storage: The session is saved to a MongoDB cloud database for persistent history.
ğŸ•µï¸ The Agents
The power of this application lies in the separation of concerns. Instead of one AI doing everything, we split the logic into three distinct roles:

1. ğŸ§  The Planner Agent
Role: The Strategist.
Function: It analyzes the conversation history to detect context.
Is this a follow-up? -> It generates queries to clarify previous points.
Is this a new topic? -> It strictly ignores previous context to prevent "hallucination merging."
Output: A list of precise search queries optimized for search engines.
2. ğŸ” The Searcher Agent
Role: The Investigator.
Function: It takes the queries from the Planner and uses the Tavily Search API.
Capabilities: It doesn't just get links; it scrapes the content, filters out ads/noise, and extracts the raw text relevant to the query.
Output: A structured block of "Verified Search Data" containing Titles, URLs, and Content snippets.
3. âœï¸ The Writer Agent
Role: The Author.
Function: It reads the raw data and drafts the final response.
Logic: It adheres to strict formatting rules:
General Mode: detailed summaries.
Academic Mode: Strict citation styles and research paper structure.
Output: A clean, Markdown-formatted report with clickable citations.
ğŸ› ï¸ Installation & Dependencies
This project relies on a modern AI stack. Here is how each dependency contributes:

Dependency	Purpose
streamlit	The frontend framework. It creates the chat interface, sidebar, and file uploader.
langgraph	Manages the "State" and control flow. It allows us to build the Planner -> Searcher -> Writer loop.
langchain	The interface for interacting with LLMs (Large Language Models) like GPT-4 or Claude.
tavily-python	A search engine specifically built for AI agents. It returns clean text instead of messy HTML.
pymongo	Connects the app to MongoDB Atlas (Cloud) to save chat history permanently.
pypdf	Extracts text from uploaded PDF research papers so the agent can "read" them.
certifi	Provides SSL certificates to ensure secure connections to the database.
ğŸ”‘ API Requirements
To run this agent, you need the following API keys:

OpenRouter API Key (OPENROUTER_API_KEY)
Why? To access powerful LLMs (like Google Gemini 2.0 Flash or Claude 3.5 Sonnet) that power the Planner and Writer agents.
Tavily API Key (TAVILY_API_KEY)
Why? To allow the Searcher agent to browse the live internet.
MongoDB Connection String (MONGO_URI)
Why? To store your research history in the cloud so it persists across sessions.
âš–ï¸ Limitations & Future Roadmap
While powerful, the agent has current limitations we aim to solve:

âŒ Current Drawbacks
Token Context Window: Extremely large PDFs (>100 pages) may be truncated because LLMs have a limit on how much text they can read at once.
Sequential Processing: The agent works in steps (Plan -> Search -> Write). If the search fails, the report may be weak. It doesn't yet "self-correct" and search again automatically.
PDF Images: It currently extracts text only. Graphs and charts inside PDFs are ignored.
ğŸš€ Future Improvements
Self-Correction Loop: Implementing a "Reviewer Node" that checks the quality of the report and sends it back to the Searcher if data is missing.
GraphRAG: Implementing a Knowledge Graph to better connect dots between different research papers.
OCR Integration: Adding a vision model to read charts and images within PDFs.
Export Options: allowing users to download reports as .docx or .pdf.
ğŸ Conclusion
The Open Deep Research Agent represents a shift from static search to agentic research. By combining the reasoning capabilities of LLMs with the real-time knowledge of the web and the persistence of a database, it offers a robust tool for students, researchers, and professionals who need deep insights fast.

ğŸ”— Access the Live Agent
Click Here to Start Researching

ğŸ’» Local Setup Guide
Clone the Repo:

Bash

git clone https://github.com/your-username/deep-research-agent.git
cd deep-research-agent


Install Requirements:

Bash

pip install -r requirements.txt
Configure Secrets:
Create a .streamlit/secrets.toml file:

toml

OPENROUTER_API_KEY = "sk-..."
TAVILY_API_KEY = "tvly-..."
MONGO_URI = "mongodb+srv://..."
Run the App:

Bash

streamlit run app.py
